{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46c307",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"whitegrid\")\n",
    "except:\n",
    "    sns = None\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e9f81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% Part 0 – Paths\n",
    "\n",
    "RUN_ID = \"YOUR_RUN_ID_HERE\"    # ← 修改为你的 run_experiment 实际 run_id\n",
    "\n",
    "WORKSPACE = Path(\"/workspace\")\n",
    "LLM_LOG_PATH = WORKSPACE / \"agent_logs\" / RUN_ID / \"llm_calls.jsonl\"\n",
    "\n",
    "HUMANEVAL_PATH = Path(\"/app/maswe/eval/humaneval.jsonl\")\n",
    "\n",
    "\n",
    "EVAL_RESULTS_PATH = WORKSPACE / \"humaneval_results\" / f\"{RUN_ID}_eval.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa98c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% Load llm_calls.jsonl\n",
    "\n",
    "def load_llm_calls(path: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            rows.append(obj)\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # 必须包含字段\n",
    "    for key in [\"task_id\", \"sample_id\", \"prompt\", \"response_raw\", \"response_code\"]:\n",
    "        if key not in df.columns:\n",
    "            print(df.columns)\n",
    "            raise ValueError(f\"Missing key in llm_calls.jsonl: {key}\")\n",
    "\n",
    "    df[\"completion_text\"] = df[\"response_code\"].fillna(\"\")\n",
    "    df[\"raw_text\"] = df[\"response_raw\"].fillna(\"\")\n",
    "    df[\"completion_char_len\"] = df[\"completion_text\"].astype(str).str.len()\n",
    "    df[\"completion_token_approx\"] = (df[\"completion_char_len\"] / 4).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "llm_df = load_llm_calls(LLM_LOG_PATH)\n",
    "llm_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c3d2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_humaneval(path=HUMANEVAL_PATH):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "humaneval_df = load_humaneval()\n",
    "humaneval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b33930",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"task_id\": \"HumanEval/0\",\n",
    "  \"pass@k\": 0.33,\n",
    "  \"num_correct\": 1,\n",
    "  \"num_total\": 3,\n",
    "  \"results\": [true, false, false]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61cd84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def explode_eval_results(path: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            task_id = obj[\"task_id\"]\n",
    "            results = obj[\"results\"]\n",
    "            for i, passed in enumerate(results):\n",
    "                rows.append({\n",
    "                    \"task_id\": task_id,\n",
    "                    \"sample_id\": i,\n",
    "                    \"passed\": bool(passed),\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "eval_df = explode_eval_results(EVAL_RESULTS_PATH)\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275566a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "merged = eval_df.merge(llm_df, on=[\"task_id\", \"sample_id\"], how=\"left\")\n",
    "\n",
    "# prompt / canonical_solution / entry_point\n",
    "merged = merged.merge(\n",
    "    humaneval_df[[\"task_id\", \"prompt\", \"canonical_solution\", \"entry_point\", \"test\"]],\n",
    "    on=\"task_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4f069",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def has_extra_output(raw: str) -> bool:\n",
    "    raw = raw.lower()\n",
    "    if \"```\" in raw:\n",
    "        return True\n",
    "    keywords = [\n",
    "        \"the function\", \"this function\", \"explanation\",\n",
    "        \"step\", \"we can\", \"in summary\", \"first,\", \"second,\", \"finally\"\n",
    "    ]\n",
    "    return any(k in raw for k in keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e33e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
